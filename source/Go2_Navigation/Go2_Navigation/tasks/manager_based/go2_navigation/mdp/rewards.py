# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from __future__ import annotations

import math
import torch
from typing import TYPE_CHECKING

from isaaclab.managers import SceneEntityCfg
from isaaclab.sensors import RayCaster

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


def position_command_error_tanh(env: ManagerBasedRLEnv, std: float, command_name: str) -> torch.Tensor:
    """Reward position tracking with tanh kernel."""
    command = env.command_manager.get_command(command_name)
    des_pos_b = command[:, :3]
    distance = torch.norm(des_pos_b, dim=1)
    return 1 - torch.tanh(distance / std)


def heading_command_error_abs(env: ManagerBasedRLEnv, command_name: str) -> torch.Tensor:
    """Penalize tracking orientation error."""
    command = env.command_manager.get_command(command_name)
    heading_b = command[:, 3]
    return heading_b.abs()


def heading_command_error_squared(env: ManagerBasedRLEnv, command_name: str) -> torch.Tensor:
    """ä½¿ç”¨å¹³æ–¹æƒ©ç½šï¼Œå¯¹å¤§åå·®æ›´æ•æ„Ÿ"""
    command = env.command_manager.get_command(command_name)
    heading_b = command[:, 3]
    return heading_b ** 2  # å¹³æ–¹æƒ©ç½š

def obstacle_safety_distance(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    safe_distance: float = 1.0,
    danger_distance: float = 0.5,
) -> torch.Tensor:
    """é™æ€éšœç¢ç‰©å®‰å…¨è·ç¦»å¥–åŠ±ã€‚
    
    è¿”å›è¿è§„ç¨‹åº¦ï¼ˆæ­£å€¼ï¼‰ï¼Œé…åˆè´Ÿweightä½¿ç”¨ä»¥å®ç°æƒ©ç½šã€‚
    - è·ç¦» >= safe_distance: è¿”å› 0 (å®‰å…¨ï¼Œæ— è¿è§„)
    - danger_distance < è·ç¦» < safe_distance: çº¿æ€§è¿è§„ (0-0.5)
    - è·ç¦» <= danger_distance: å¼ºè¿è§„ï¼ˆæŒ‡æ•°å¢é•¿ï¼Œ0.5-1.0ï¼‰
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        RewTerm(func=mdp.obstacle_safety_distance, weight=-50.0, ...)
        â†’ æ¥è¿‘éšœç¢ç‰©æ—¶è·å¾—è´Ÿå¥–åŠ±ï¼ˆæƒ©ç½šï¼‰
    
    Args:
        env: The environment.
        sensor_cfg: The raycaster sensor configuration (obstacle_scanner).
        safe_distance: å®‰å…¨è·ç¦»é˜ˆå€¼ï¼ˆç±³ï¼‰ã€‚è¶…è¿‡æ­¤è·ç¦»æ— è¿è§„ã€‚Defaults to 1.0.
        danger_distance: å±é™©è·ç¦»é˜ˆå€¼ï¼ˆç±³ï¼‰ã€‚ä½äºæ­¤è·ç¦»å¼ºè¿è§„ã€‚Defaults to 0.5.
        
    Returns:
        Violation score for each environment. Shape: (num_envs,).
        è¿”å›å€¼ä¸ºæ­£ï¼ˆè¿è§„ç¨‹åº¦ï¼‰ï¼Œè·ç¦»è¶Šè¿‘å€¼è¶Šå¤§ï¼Œé…åˆè´Ÿweightå®ç°æƒ©ç½šã€‚
    """
    # è·å–RayCasterä¼ æ„Ÿå™¨
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    
    # è·å–æ‰€æœ‰å°„çº¿çš„è·ç¦»
    ray_hits_w = sensor.data.ray_hits_w  # (num_envs, num_rays, 3)
    sensor_pos_w = sensor.data.pos_w.unsqueeze(1)  # (num_envs, 1, 3)
    
    # è®¡ç®—åˆ°éšœç¢ç‰©çš„è·ç¦»
    distances = torch.norm(ray_hits_w - sensor_pos_w, dim=-1)  # (num_envs, num_rays)
    
    # å¤„ç†infï¼ˆæœªå‘½ä¸­çš„å°„çº¿è§†ä¸ºå®‰å…¨è·ç¦»ï¼‰
    distances = torch.where(
        torch.isinf(distances),
        torch.full_like(distances, safe_distance * 2),  # è¿œå¤§äºå®‰å…¨è·ç¦»
        distances
    )
    
    # å–æœ€å°è·ç¦»ï¼ˆæœ€å±é™©çš„éšœç¢ç‰©ï¼‰
    min_distances = torch.min(distances, dim=1)[0]  # (num_envs,)
    
    # è®¡ç®—è¿è§„ç¨‹åº¦ï¼ˆåˆ†æ®µå‡½æ•°ï¼Œè¿”å›æ­£å€¼ï¼‰
    violation = torch.zeros_like(min_distances)
    
    # å±é™©åŒºåŸŸï¼ˆd < danger_distanceï¼‰ï¼šæŒ‡æ•°è¿è§„ (0.5-1.0)
    danger_mask = min_distances < danger_distance
    if danger_mask.any():
        # æŒ‡æ•°å‡½æ•°ï¼šè·ç¦»è¶Šè¿‘ï¼Œè¿è§„è¶Šå¤§
        ratio = min_distances[danger_mask] / danger_distance
        # å°†åŸæ¥çš„[-1, ~0]æ˜ å°„åˆ°[1.0, 0.5]
        violation[danger_mask] = torch.exp(-2.0 * ratio) - math.exp(-2.0) + 1.0  # èŒƒå›´: [0.5, 1.0]
    
    # è­¦å‘ŠåŒºåŸŸï¼ˆdanger_distance <= d < safe_distanceï¼‰ï¼šçº¿æ€§è¿è§„ (0-0.5)
    warning_mask = (min_distances >= danger_distance) & (min_distances < safe_distance)
    if warning_mask.any():
        # çº¿æ€§æ’å€¼ï¼šè·ç¦»è¶Šè¿‘è¿è§„è¶Šå¤§
        ratio = (min_distances[warning_mask] - danger_distance) / (safe_distance - danger_distance)
        violation[warning_mask] = 0.5 * (1.0 - ratio)  # èŒƒå›´: [0, 0.5]
    
    # å®‰å…¨åŒºåŸŸï¼ˆd >= safe_distanceï¼‰ï¼šæ— è¿è§„ (violation = 0)
    
    return violation


def obstacle_proximity_penalty(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    threshold: float = 0.8,
    kernel: str = "exp",
) -> torch.Tensor:
    """åŸºäºæ‰€æœ‰æ‰‡åŒºçš„éšœç¢ç‰©æ¥è¿‘åº¦æƒ©ç½šï¼ˆæ›´å¹³æ»‘çš„ç‰ˆæœ¬ï¼‰ã€‚
    
    è€ƒè™‘æ‰€æœ‰æ–¹å‘çš„éšœç¢ç‰©ï¼Œä½¿ç”¨å¹³å‡è·ç¦»è€Œä¸æ˜¯æœ€å°è·ç¦»ã€‚
    
    Args:
        env: The environment.
        sensor_cfg: The raycaster sensor configuration.
        threshold: è·ç¦»é˜ˆå€¼ï¼ˆç±³ï¼‰ï¼Œä½äºæ­¤å€¼å¼€å§‹æƒ©ç½šã€‚Defaults to 0.8.
        kernel: æƒ©ç½šæ ¸å‡½æ•°ç±»å‹ ("exp" æˆ– "linear")ã€‚Defaults to "exp".
        
    Returns:
        Proximity penalty. Shape: (num_envs,).
    """
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    
    # è®¡ç®—è·ç¦»
    ray_hits_w = sensor.data.ray_hits_w
    sensor_pos_w = sensor.data.pos_w.unsqueeze(1)
    distances = torch.norm(ray_hits_w - sensor_pos_w, dim=-1)
    
    # å¤„ç†inf
    distances = torch.where(
        torch.isinf(distances),
        torch.full_like(distances, threshold * 2),
        distances
    )
    
    # è®¡ç®—å¹³å‡å‰æ–¹è·ç¦»ï¼ˆæƒé‡å¹³å‡ï¼‰
    # å¯ä»¥è€ƒè™‘åªè®¡ç®—å‰åŠçƒçš„å°„çº¿
    mean_distance = torch.mean(distances, dim=1)
    
    # è®¡ç®—æƒ©ç½š
    if kernel == "exp":
        # æŒ‡æ•°æ ¸ï¼šè·ç¦»è¶Šè¿‘ï¼Œæƒ©ç½šå¢é•¿è¶Šå¿«
        violation = torch.clamp(threshold - mean_distance, min=0.0)
        penalty = -violation * torch.exp(violation / threshold)
    else:  # linear
        # çº¿æ€§æ ¸ï¼šç®€å•çº¿æ€§æƒ©ç½š
        violation = torch.clamp(threshold - mean_distance, min=0.0)
        penalty = -violation / threshold
    
    return penalty

def time_efficiency_bonus(env: ManagerBasedRLEnv, command_name: str) -> torch.Tensor:
    """
    å¥–åŠ±å¿«é€Ÿæ¥è¿‘ç›®æ ‡çš„é€»è¾‘è®¾è®¡å¦‚ä¸‹ï¼š

    - å¥–åŠ±ç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼šé è¿‘ç›®æ ‡çš„ç¨‹åº¦ï¼ˆè·ç¦»ï¼‰å’Œæ¥è¿‘ç›®æ ‡çš„é€Ÿåº¦ï¼ˆé€Ÿåº¦æ–¹å‘ï¼‰ã€‚
    - è·ç¦»ç›®æ ‡è¶Šè¿‘ï¼Œå¥–åŠ±è¶Šå¤§ï¼ˆç”¨è·ç¦»çš„è´ŸæŒ‡æ•°å‡½æ•°åšåŠ æƒï¼Œè¶Šè¿‘æƒé‡è¶Šå¤§ï¼‰ã€‚
    - åªæœ‰å½“æœºå™¨äººæœå‘ç›®æ ‡ç§»åŠ¨æ—¶ï¼Œé€Ÿåº¦è¶Šå¤§å¥–åŠ±è¶Šå¤§ï¼ˆç”¨é€Ÿåº¦åœ¨ç›®æ ‡æ–¹å‘ä¸Šçš„æŠ•å½±ï¼‰ã€‚
    - è¿™æ ·é¼“åŠ±æœºå™¨äººæ—¢è¦é è¿‘ç›®æ ‡ï¼Œä¹Ÿè¦ä»¥é«˜æ•ˆçš„é€Ÿåº¦æœç›®æ ‡å‰è¿›ã€‚

    Args:
        env: ç¯å¢ƒå¯¹è±¡ã€‚
        command_name: ç›®æ ‡å‘½ä»¤çš„åç§°ã€‚

    Returns:
        å¥–åŠ±å€¼ï¼Œshape: (num_envs,)
    """
    # ç›®æ ‡åœ¨baseåæ ‡ç³»ä¸‹çš„ä½ç½®
    command = env.command_manager.get_command(command_name)  # (num_envs, 3/4)
    des_pos_b = command[:, :2]  # åªå–x, yå¹³é¢
    distance = torch.norm(des_pos_b, dim=1)  # (num_envs,)

    # æœºå™¨äººåœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„çº¿é€Ÿåº¦ï¼ˆx, yï¼‰
    vel_w = env.scene["robot"].data.root_lin_vel_w[:, :2]  # (num_envs, 2)
    # æœºå™¨äººæœå‘ç›®æ ‡çš„å•ä½å‘é‡ï¼ˆåœ¨baseç³»ï¼Œéœ€è½¬åˆ°ä¸–ç•Œç³»ï¼‰
    # è¿™é‡Œå‡è®¾ç›®æ ‡å‘é‡å·²åœ¨baseç³»ï¼Œéœ€è½¬åˆ°ä¸–ç•Œç³»ã€‚ç®€åŒ–ï¼šå‡è®¾baseæœå‘ä¸ä¸–ç•Œå¯¹é½ã€‚
    # æ›´ä¸¥è°¨åšæ³•ï¼šéœ€ç”¨baseçš„æœå‘æ—‹è½¬des_pos_båˆ°ä¸–ç•Œç³»ã€‚
    # ä½†å¦‚æœç›®æ ‡å‘½ä»¤æœ¬èº«å°±æ˜¯åœ¨baseç³»ä¸‹çš„ç›¸å¯¹å‘é‡ï¼Œåˆ™ç›´æ¥ç”¨å³å¯ã€‚
    direction_to_goal = torch.nn.functional.normalize(des_pos_b, dim=1)  # (num_envs, 2)
    # é€Ÿåº¦åœ¨ç›®æ ‡æ–¹å‘ä¸Šçš„æŠ•å½±
    velocity_towards_goal = (vel_w * direction_to_goal).sum(dim=1)  # (num_envs,)

    # è·ç¦»åŠ æƒï¼ˆè·ç¦»è¶Šè¿‘ï¼Œæƒé‡è¶Šå¤§ï¼‰
    proximity_weight = torch.exp(-distance / 2.0)  # (num_envs,)

    # å¥–åŠ± = é€Ÿåº¦æŠ•å½± * è·ç¦»åŠ æƒ * ç³»æ•°
    reward = velocity_towards_goal * proximity_weight * 0.1

    return reward


def goal_reached_bonus(
    env: ManagerBasedRLEnv,
    command_name: str,
    distance_threshold: float = 0.3,
) -> torch.Tensor:
    """æˆåŠŸåˆ°è¾¾ç›®æ ‡ç‚¹çš„å¥–åŠ±ã€‚
    
    å½“æœºå™¨äººè¿›å…¥ç›®æ ‡åŒºåŸŸï¼ˆè·ç¦»<thresholdï¼‰æ—¶ç»™äºˆä¸€æ¬¡æ€§å¤§é¢å¥–åŠ±ã€‚
    è¿™ä¸ªå¥–åŠ±ä¼šåœ¨ç»ˆæ­¢æ—¶è§¦å‘ï¼Œé¼“åŠ±ç­–ç•¥å°½å¿«åˆ°è¾¾ç›®æ ‡ã€‚
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        command_name: å‘½ä»¤åç§°
        distance_threshold: è®¤ä¸ºåˆ°è¾¾ç›®æ ‡çš„è·ç¦»é˜ˆå€¼ï¼ˆç±³ï¼‰
        
    Returns:
        å¥–åŠ±å¼ é‡ï¼šåˆ°è¾¾ç›®æ ‡åŒºåŸŸå†…çš„ç¯å¢ƒè¿”å›1.0ï¼Œå¦åˆ™è¿”å›0.0
    """
    # è·å–ç›®æ ‡ä½ç½®å‘½ä»¤
    command = env.command_manager.get_command(command_name)
    target_pos_b = command[:, :3]  # ç›®æ ‡ä½ç½®ï¼ˆæœºå™¨äººåæ ‡ç³»ï¼‰
    
    # è®¡ç®—åˆ°ç›®æ ‡çš„è·ç¦»ï¼ˆåªè€ƒè™‘xyå¹³é¢ï¼‰
    distance = torch.norm(target_pos_b[:, :2], dim=1)
    
    # å¦‚æœè·ç¦»å°äºé˜ˆå€¼ï¼Œè¿”å›1.0ï¼ˆæˆåŠŸåˆ°è¾¾ï¼‰ï¼Œå¦åˆ™è¿”å›0.0
    return (distance < distance_threshold).float()


def goal_reached_bonus_time_aware(
    env: ManagerBasedRLEnv,
    command_name: str,
    distance_threshold: float = 0.3,
    base_reward: float = 100.0,
    time_bonus_weight: float = 100.0,
) -> torch.Tensor:
    """æ—¶é—´æ„ŸçŸ¥çš„æˆåŠŸåˆ°è¾¾å¥–åŠ±ã€‚
    
    é¼“åŠ±æœºå™¨äººå°½å¿«åˆ°è¾¾ç›®æ ‡ï¼š
    - å¿«é€Ÿåˆ°è¾¾ï¼ˆå‰©ä½™æ—¶é—´å¤šï¼‰â†’ é«˜å¥–åŠ±
    - æ…¢é€Ÿåˆ°è¾¾ï¼ˆå‰©ä½™æ—¶é—´å°‘ï¼‰â†’ ä½å¥–åŠ±
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        command_name: å‘½ä»¤åç§°
        distance_threshold: è®¤ä¸ºåˆ°è¾¾ç›®æ ‡çš„è·ç¦»é˜ˆå€¼ï¼ˆç±³ï¼‰
        base_reward: åŸºç¡€å¥–åŠ±ï¼ˆæ€»æ˜¯ç»™äºˆï¼‰
        time_bonus_weight: æ—¶é—´å¥–åŠ±æƒé‡ï¼ˆæ ¹æ®å‰©ä½™æ—¶é—´è®¡ç®—ï¼‰
        
    Returns:
        å¥–åŠ±å¼ é‡ï¼šåˆ°è¾¾æ—¶ = base_reward + time_bonusï¼Œæœªåˆ°è¾¾ = 0
        
    ç¤ºä¾‹ï¼š
        episode_length = 12ç§’ = 60æ­¥
        
        6ç§’åˆ°è¾¾ï¼ˆ30æ­¥ï¼‰ï¼š
          time_ratio = 30/60 = 0.5
          reward = 100 + (1-0.5)*100 = 150 âœ… å¿«é€Ÿå¥–åŠ±
          
        11ç§’åˆ°è¾¾ï¼ˆ55æ­¥ï¼‰ï¼š
          time_ratio = 55/60 = 0.92
          reward = 100 + (1-0.92)*100 = 108 ğŸŸ¡ æ…¢é€Ÿå¥–åŠ±
    """
    # è·å–ç›®æ ‡ä½ç½®å‘½ä»¤
    command = env.command_manager.get_command(command_name)
    target_pos_b = command[:, :3]
    
    # è®¡ç®—åˆ°ç›®æ ‡çš„è·ç¦»ï¼ˆåªè€ƒè™‘xyå¹³é¢ï¼‰
    distance = torch.norm(target_pos_b[:, :2], dim=1)
    reached = (distance < distance_threshold).float()
    
    # è®¡ç®—å·²ç”¨æ—¶é—´æ¯”ä¾‹ (0-1)
    time_ratio = env.episode_length_buf.float() / env.max_episode_length
    
    # è®¡ç®—æ—¶é—´å¥–åŠ±ï¼šå‰©ä½™æ—¶é—´è¶Šå¤šï¼Œå¥–åŠ±è¶Šé«˜
    time_bonus = (1.0 - time_ratio) * time_bonus_weight
    
    # æ€»å¥–åŠ± = åŸºç¡€å¥–åŠ± + æ—¶é—´å¥–åŠ±ï¼ˆä»…åœ¨åˆ°è¾¾æ—¶ç»™äºˆï¼‰
    return reached * (base_reward + time_bonus)


def velocity_smoothness_penalty(env: ManagerBasedRLEnv) -> torch.Tensor:
    """é€Ÿåº¦å¹³æ»‘æ€§æƒ©ç½šã€‚
    
    è¿”å›è¿è§„ç¨‹åº¦ï¼ˆæ­£å€¼ï¼‰ï¼Œé…åˆè´Ÿweightä½¿ç”¨ä»¥å®ç°æƒ©ç½šã€‚
    - è®¡ç®—å½“å‰é€Ÿåº¦ä¸ä¸Šä¸€å¸§é€Ÿåº¦çš„å·®å€¼
    - å·®å€¼è¶Šå¤§ï¼Œè¿è§„ç¨‹åº¦è¶Šé«˜
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        RewTerm(func=mdp.velocity_smoothness_penalty, weight=-0.1, ...)
        â†’ é€Ÿåº¦çªå˜æ—¶è·å¾—è´Ÿå¥–åŠ±ï¼ˆæƒ©ç½šï¼‰
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        
    Returns:
        è¿è§„ç¨‹åº¦å¼ é‡ï¼šshape (num_envs,)ï¼Œå€¼ä¸ºæ­£ï¼ˆåŠ é€Ÿåº¦å¤§å°ï¼‰
        
    æ³¨æ„ï¼š
        è¿™ä¸ªå‡½æ•°éœ€è¦ç¯å¢ƒå­˜å‚¨ä¸Šä¸€å¸§çš„é€Ÿåº¦ã€‚
        å¦‚æœæ˜¯ç¬¬ä¸€æ­¥ï¼ˆæ²¡æœ‰å†å²ï¼‰ï¼Œè¿”å›0ï¼ˆæ— è¿è§„ï¼‰ã€‚
    """
    robot = env.scene["robot"]
    current_vel = robot.data.root_lin_vel_b[:, :2]  # å½“å‰çº¿é€Ÿåº¦ (x, y)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å†å²é€Ÿåº¦è®°å½•
    if not hasattr(env, '_last_lin_vel'):
        # ç¬¬ä¸€æ­¥ï¼Œåˆå§‹åŒ–å†å²é€Ÿåº¦
        env._last_lin_vel = current_vel.clone()
        return torch.zeros(env.num_envs, device=env.device)
    
    # è®¡ç®—é€Ÿåº¦å˜åŒ–ï¼ˆåŠ é€Ÿåº¦çš„è¿‘ä¼¼ï¼‰
    vel_change = current_vel - env._last_lin_vel
    acceleration = torch.norm(vel_change, dim=1)  # L2èŒƒæ•°
    
    # æ›´æ–°å†å²é€Ÿåº¦
    env._last_lin_vel = current_vel.clone()
    
    # è¿”å›æ­£å€¼ï¼ˆè¿è§„ç¨‹åº¦ï¼‰ï¼Œé…åˆè´Ÿweightå®ç°æƒ©ç½š
    return acceleration


def time_efficiency_bonus_fixed(env: ManagerBasedRLEnv, command_name: str) -> torch.Tensor:
    """ä¿®æ­£ç‰ˆçš„æ—¶é—´æ•ˆç‡å¥–åŠ±ï¼ˆæ­£ç¡®çš„åæ ‡ç³»è½¬æ¢ï¼‰ã€‚
    
    å¥–åŠ±æœå‘ç›®æ ‡å¿«é€Ÿç§»åŠ¨çš„è¡Œä¸ºï¼š
    - é€Ÿåº¦åœ¨ç›®æ ‡æ–¹å‘çš„æŠ•å½±è¶Šå¤§ï¼Œå¥–åŠ±è¶Šé«˜
    - è·ç¦»ç›®æ ‡è¶Šè¿‘ï¼Œæƒé‡è¶Šå¤§ï¼ˆé¼“åŠ±æ¥è¿‘æ—¶ä¿æŒé€Ÿåº¦ï¼‰
    
    Args:
        env: ç¯å¢ƒå¯¹è±¡
        command_name: ç›®æ ‡å‘½ä»¤çš„åç§°
        
    Returns:
        å¥–åŠ±å€¼ï¼Œshape: (num_envs,)
        
    ä¿®æ­£å†…å®¹ï¼š
        æ­£ç¡®å¤„ç†åæ ‡ç³»è½¬æ¢ï¼Œå°†ç›®æ ‡æ–¹å‘ä»æœºå™¨äººåæ ‡ç³»è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³»
    """
    # è·å–ç›®æ ‡ä½ç½®ï¼ˆæœºå™¨äººåæ ‡ç³»ï¼‰
    command = env.command_manager.get_command(command_name)
    des_pos_b = command[:, :2]  # (num_envs, 2) - æœºå™¨äººåæ ‡ç³»
    distance = torch.norm(des_pos_b, dim=1)
    
    # è·å–æœºå™¨äººçŠ¶æ€
    robot = env.scene["robot"]
    yaw = robot.data.heading_w  # (num_envs,) - ä¸–ç•Œåæ ‡ç³»ä¸­çš„æœå‘è§’
    
    # å°†ç›®æ ‡æ–¹å‘ä»æœºå™¨äººåæ ‡ç³»è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³»
    cos_yaw = torch.cos(yaw)
    sin_yaw = torch.sin(yaw)
    
    # æ—‹è½¬çŸ©é˜µï¼šR = [[cos, -sin], [sin, cos]]
    direction_w_x = des_pos_b[:, 0] * cos_yaw - des_pos_b[:, 1] * sin_yaw
    direction_w_y = des_pos_b[:, 0] * sin_yaw + des_pos_b[:, 1] * cos_yaw
    direction_w = torch.stack([direction_w_x, direction_w_y], dim=1)
    direction_w = torch.nn.functional.normalize(direction_w, dim=1)
    
    # é€Ÿåº¦åœ¨ç›®æ ‡æ–¹å‘çš„æŠ•å½±ï¼ˆç°åœ¨éƒ½åœ¨ä¸–ç•Œåæ ‡ç³»ï¼‰
    vel_w = robot.data.root_lin_vel_w[:, :2]  # (num_envs, 2) - ä¸–ç•Œåæ ‡ç³»
    velocity_towards_goal = (vel_w * direction_w).sum(dim=1)  # æ ‡é‡ç§¯
    
    # è·ç¦»åŠ æƒï¼ˆè·ç¦»è¶Šè¿‘ï¼Œæƒé‡è¶Šå¤§ï¼‰
    proximity_weight = torch.exp(-distance / 2.0)
    
    # å¥–åŠ± = é€Ÿåº¦æŠ•å½± * è·ç¦»åŠ æƒ * ç³»æ•°
    return velocity_towards_goal * proximity_weight * 0.1


def goal_progress(env: ManagerBasedRLEnv, command_name: str) -> torch.Tensor:
    """å¥–åŠ±å‘ç›®æ ‡å‰è¿›çš„è¿›åº¦ï¼ˆè·ç¦»å‡å°‘é‡ï¼‰ã€‚
    
    æ ¸å¿ƒæ€æƒ³ï¼šæ¯ä¸€æ­¥å¦‚æœç¼©çŸ­äº†ä¸ç›®æ ‡çš„è·ç¦»ï¼Œå°±ç»™æ­£å¥–åŠ±ã€‚
    - è·ç¦»å‡å°‘ â†’ æ­£å¥–åŠ±ï¼ˆé¼“åŠ±ï¼‰
    - è·ç¦»å¢åŠ  â†’ è´Ÿå¥–åŠ±ï¼ˆæƒ©ç½šï¼‰
    - è·ç¦»ä¸å˜ â†’ é›¶å¥–åŠ±
    
    è¿™æ˜¯ä¸€ä¸ªå¯†é›†å¥–åŠ±ï¼Œç›´æ¥åæ˜ ç­–ç•¥çš„æ•ˆæœã€‚
    
    Args:
        env: ç¯å¢ƒ
        command_name: ç›®æ ‡å‘½ä»¤åç§°
        
    Returns:
        å¥–åŠ±ï¼šè·ç¦»å‡å°‘é‡ï¼ˆç±³ï¼‰ï¼Œæ­£å€¼=é è¿‘ï¼Œè´Ÿå€¼=è¿œç¦»
    """
    # è·å–å½“å‰åˆ°ç›®æ ‡çš„è·ç¦»
    command = env.command_manager.get_command(command_name)
    current_distance = torch.norm(command[:, :2], dim=1)  # åªçœ‹xyå¹³é¢
    
    # å­˜å‚¨é”®
    storage_key = f"previous_distance_{command_name}"
    
    # ğŸ”§ æ£€æŸ¥episodeé‡ç½®ï¼šå¦‚æœresetæ ‡å¿—å­˜åœ¨ï¼Œæ¸…é™¤å†å²
    if hasattr(env, 'episode_length_buf'):
        # episode_length_buf=0 è¡¨ç¤ºåˆšåˆšé‡ç½®
        reset_mask = env.episode_length_buf == 0
        if reset_mask.any():
            if storage_key in env.extras:
                # é‡ç½®å¯¹åº”ç¯å¢ƒçš„å†å²è·ç¦»
                env.extras[storage_key][reset_mask] = current_distance[reset_mask]
    
    # åˆå§‹åŒ–ï¼šå¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œå­˜å‚¨å½“å‰è·ç¦»
    if storage_key not in env.extras:
        env.extras[storage_key] = current_distance.clone()
        return torch.zeros_like(current_distance)  # ç¬¬ä¸€æ­¥æ²¡æœ‰å†å²ï¼Œè¿”å›0
    
    # è·å–ä¸Šä¸€æ­¥çš„è·ç¦»
    previous_distance = env.extras[storage_key]
    
    # è®¡ç®—è¿›åº¦ï¼šè·ç¦»å‡å°‘é‡
    # positive = é è¿‘ç›®æ ‡ï¼ˆå¥½ï¼‰
    # negative = è¿œç¦»ç›®æ ‡ï¼ˆå·®ï¼‰
    progress = previous_distance - current_distance
    
    # æ›´æ–°å­˜å‚¨ï¼ˆä¸ºä¸‹ä¸€æ­¥å‡†å¤‡ï¼‰
    env.extras[storage_key] = current_distance.clone()
    
    # æ”¾å¤§ç³»æ•°ï¼šå°†ç±³è½¬æ¢ä¸ºåˆé€‚çš„å¥–åŠ±å°ºåº¦
    # ä¾‹å¦‚ï¼šç¼©çŸ­0.1ç±³ â†’ å¥–åŠ±1.0
    return progress * 10.0


def heading_alignment(env: ManagerBasedRLEnv, command_name: str) -> torch.Tensor:
    """å¥–åŠ±æœºå™¨äººæœå‘ä¸ç›®æ ‡æ–¹å‘çš„å¯¹é½ç¨‹åº¦ã€‚
    
    ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼šæœå‘å®Œå…¨å¯¹é½=1.0ï¼Œå®Œå…¨ç›¸å=-1.0
    ç›¸æ¯”heading_command_errorç³»åˆ—ï¼ˆåŸºäºè§’åº¦è¯¯å·®ï¼‰ï¼Œè¿™ä¸ªä½¿ç”¨ä½™å¼¦æ›´å¹³æ»‘ã€‚
    
    Args:
        env: ç¯å¢ƒ
        command_name: ç›®æ ‡å‘½ä»¤åç§°
        
    Returns:
        å¥–åŠ±ï¼šæœå‘å¯¹é½åº¦ï¼ˆ0åˆ°1ä¹‹é—´ï¼Œ1è¡¨ç¤ºå®Œå…¨å¯¹é½ï¼‰
    """
    # è·å–ç›®æ ‡ä½ç½®ï¼ˆæœºå™¨äººåæ ‡ç³»ï¼‰
    command = env.command_manager.get_command(command_name)
    des_pos_b = command[:, :2]  # (num_envs, 2)
    
    # è®¡ç®—åˆ°ç›®æ ‡çš„è§’åº¦ï¼ˆæœºå™¨äººåæ ‡ç³»ä¸­ï¼‰
    # atan2(y, x)ï¼šxè½´=å‰æ–¹ï¼Œyè½´=å·¦ä¾§
    angle_to_goal = torch.atan2(des_pos_b[:, 1], des_pos_b[:, 0])
    
    # è®¡ç®—æœå‘å¯¹é½åº¦ï¼ˆä½¿ç”¨ä½™å¼¦ï¼‰
    # cos(0) = 1.0 (å®Œå…¨å¯¹é½)
    # cos(Ï€) = -1.0 (å®Œå…¨ç›¸å)
    alignment = torch.cos(angle_to_goal)
    
    # æ˜ å°„åˆ° [0, 1]ï¼šå®Œå…¨å¯¹é½=1ï¼Œå‚ç›´=0.5ï¼Œç›¸å=0
    alignment_normalized = (alignment + 1.0) / 2.0
    
    return alignment_normalized


def safe_velocity_near_obstacles(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    safe_distance: float = 2.0,
    danger_distance: float = 1.0,
    safe_speed: float = 1.5,
) -> torch.Tensor:
    """å¥–åŠ±æœºå™¨äººåœ¨æ¥è¿‘éšœç¢ç‰©æ—¶å‡é€Ÿã€‚
    
    è¿”å›è¿è§„ç¨‹åº¦ï¼ˆæ­£å€¼ï¼‰ï¼Œé…åˆè´Ÿweightä½¿ç”¨ä»¥å®ç°æƒ©ç½šã€‚
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - ç¦»éšœç¢ç‰©è¿œæ—¶ï¼ˆ>safe_distanceï¼‰ï¼Œå¯ä»¥å…¨é€Ÿå‰è¿› â†’ è¿”å›0
    - æ¥è¿‘éšœç¢ç‰©æ—¶ï¼ˆ<safe_distanceï¼‰ï¼Œé€Ÿåº¦åº”è¯¥é™ä½ â†’ è¶…é€Ÿè¿”å›æ­£å€¼
    - å¾ˆè¿‘æ—¶ï¼ˆ<danger_distanceï¼‰ï¼Œåº”è¯¥å‡ ä¹åœæ­¢ â†’ è¶…é€Ÿæƒ©ç½šæ›´å¤§
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        RewTerm(func=mdp.safe_velocity_near_obstacles, weight=-1.0, ...)
        â†’ æ¥è¿‘éšœç¢ç‰©æ—¶è¶…é€Ÿä¼šè·å¾—è´Ÿå¥–åŠ±ï¼ˆæƒ©ç½šï¼‰
    
    Args:
        env: ç¯å¢ƒ
        sensor_cfg: ä¼ æ„Ÿå™¨é…ç½®
        asset_cfg: æœºå™¨äººasseté…ç½®
        safe_distance: å®‰å…¨è·ç¦»ï¼ˆç±³ï¼‰ï¼Œè¶…è¿‡æ­¤è·ç¦»å¯å…¨é€Ÿ
        danger_distance: å±é™©è·ç¦»ï¼ˆç±³ï¼‰ï¼Œå°äºæ­¤è·ç¦»åº”å‡é€Ÿ
        safe_speed: å®‰å…¨é€Ÿåº¦ä¸Šé™ï¼ˆm/sï¼‰
        
    Returns:
        è¿è§„ç¨‹åº¦ï¼šé€Ÿåº¦è¶…é™æ—¶çš„è¿è§„ç¨‹åº¦ï¼ˆæ­£å€¼ï¼‰
    """
    # è·å–éšœç¢ç‰©è·ç¦»æ•°æ®
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    ray_distances = sensor.data.ray_hits_w[..., 0]  # (num_envs, num_rays)
    
    # æ‰¾åˆ°æœ€è¿‘çš„éšœç¢ç‰©è·ç¦»
    min_distance = torch.min(ray_distances, dim=1)[0]  # (num_envs,)
    
    # è·å–å½“å‰é€Ÿåº¦
    asset = env.scene[asset_cfg.name]
    velocity = asset.data.root_lin_vel_w
    speed = torch.norm(velocity[:, :2], dim=1)  # xyå¹³é¢é€Ÿåº¦
    
    # è®¡ç®—å±é™©åº¦ï¼ˆ0-1ï¼Œ1=éå¸¸å±é™©ï¼‰
    # danger_level = 1.0 when distance < danger_distance
    # danger_level = 0.0 when distance > safe_distance
    danger_level = torch.clamp(
        (safe_distance - min_distance) / (safe_distance - danger_distance),
        0.0,
        1.0,
    )
    
    # è®¡ç®—æœŸæœ›çš„å®‰å…¨é€Ÿåº¦
    # å½“danger_level=1æ—¶ï¼ŒæœŸæœ›é€Ÿåº¦=0.3m/sï¼ˆå‡ ä¹åœæ­¢ï¼‰
    # å½“danger_level=0æ—¶ï¼ŒæœŸæœ›é€Ÿåº¦=safe_speedï¼ˆæ­£å¸¸é€Ÿåº¦ï¼‰
    desired_speed = safe_speed * (1.0 - danger_level * 0.8)  # æœ€ä½é™åˆ°20%é€Ÿåº¦
    
    # å¦‚æœå®é™…é€Ÿåº¦è¶…è¿‡æœŸæœ›é€Ÿåº¦ï¼Œè®¡ç®—è¿è§„ç¨‹åº¦
    speed_violation = torch.clamp(speed - desired_speed, min=0.0)
    
    # è¿è§„ç¨‹åº¦ = é€Ÿåº¦è¿è§„é‡ * å±é™©åº¦ï¼ˆè¿”å›æ­£å€¼ï¼‰
    violation = speed_violation * danger_level * 2.0
    
    return violation


def heading_towards_velocity(
    env: ManagerBasedRLEnv, 
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    velocity_threshold: float = 0.2
) -> torch.Tensor:
    """å¥–åŠ±æœºå™¨äººæœå‘å…¶é€Ÿåº¦æ–¹å‘ï¼ˆè€Œä¸æ˜¯ç›®æ ‡æ–¹å‘ï¼‰ã€‚
    
    è®©æœºå™¨äººçš„å¤´éƒ¨æœå‘è¿åŠ¨æ–¹å‘ï¼Œè¿™æ ·å¯ä»¥ï¼š
    1. æ›´è‡ªç„¶çš„è¿åŠ¨ï¼ˆå››è¶³åŠ¨ç‰©é€šå¸¸æœå‘è¿åŠ¨æ–¹å‘ï¼‰
    2. æ›´é«˜æ•ˆçš„é¿éšœï¼ˆå¯ä»¥ä¾§å‘/åé€€ç§»åŠ¨ï¼‰
    3. æ›´çµæ´»çš„å¯¼èˆªç­–ç•¥
    
    Args:
        env: ç¯å¢ƒ
        asset_cfg: æœºå™¨äººasseté…ç½®
        velocity_threshold: é€Ÿåº¦é˜ˆå€¼ï¼ˆm/sï¼‰ï¼Œä½äºæ­¤é€Ÿåº¦ä¸è®¡ç®—å¥–åŠ±ï¼ˆé¿å…åŸåœ°æ‰“è½¬æ—¶çš„å™ªå£°ï¼‰
        
    Returns:
        å¥–åŠ±ï¼šæœå‘ä¸é€Ÿåº¦æ–¹å‘çš„å¯¹é½åº¦ï¼ˆ0åˆ°1ï¼Œ1è¡¨ç¤ºå®Œå…¨å¯¹é½ï¼‰
    """
    # è·å–asset
    asset = env.scene[asset_cfg.name]
    
    # è·å–æœºå™¨äººåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„çº¿é€Ÿåº¦ (num_envs, 3)
    velocity_w = asset.data.root_lin_vel_w
    velocity_xy = velocity_w[:, :2]  # åªçœ‹xyå¹³é¢çš„é€Ÿåº¦
    
    # è®¡ç®—é€Ÿåº¦å¤§å°
    speed = torch.norm(velocity_xy, dim=1)
    
    # è·å–æœºå™¨äººæœå‘ï¼ˆyawè§’ï¼‰
    robot_heading = asset.data.heading_w  # (num_envs,)
    
    # è®¡ç®—é€Ÿåº¦æ–¹å‘è§’åº¦
    velocity_angle = torch.atan2(velocity_xy[:, 1], velocity_xy[:, 0])  # (num_envs,)
    
    # è®¡ç®—æœå‘ä¸é€Ÿåº¦æ–¹å‘çš„å¤¹è§’
    angle_diff = velocity_angle - robot_heading
    
    # å½’ä¸€åŒ–è§’åº¦åˆ°[-Ï€, Ï€]
    angle_diff = torch.atan2(torch.sin(angle_diff), torch.cos(angle_diff))
    
    # è®¡ç®—å¯¹é½åº¦ï¼šcos(angle_diff)
    # cos(0) = 1.0 (å®Œå…¨å¯¹é½)
    # cos(Ï€) = -1.0 (å®Œå…¨ç›¸å)
    alignment = torch.cos(angle_diff)
    
    # æ˜ å°„åˆ° [0, 1]
    alignment_normalized = (alignment + 1.0) / 2.0
    
    # åªåœ¨é€Ÿåº¦è¶³å¤Ÿå¤§æ—¶æ‰ç»™å¥–åŠ±ï¼ˆé¿å…åŸåœ°æ‰“è½¬æ—¶çš„å™ªå£°ï¼‰
    mask = speed > velocity_threshold
    reward = torch.where(mask, alignment_normalized, torch.zeros_like(alignment_normalized))
    
    return reward


def backward_motion_penalty(
    env: ManagerBasedRLEnv, 
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
) -> torch.Tensor:
    """æƒ©ç½šåé€€è¿åŠ¨ï¼ˆå€’ç€èµ°ï¼‰ã€‚
    
    è®¡ç®—æœºå™¨äººé€Ÿåº¦åœ¨å…¶æœå‘æ–¹å‘ä¸Šçš„æŠ•å½±ï¼š
    - å¦‚æœæŠ•å½±ä¸ºè´Ÿï¼ˆåé€€ï¼‰ï¼Œè¿”å›æ­£çš„æƒ©ç½šå€¼
    - å¦‚æœæŠ•å½±ä¸ºæ­£ï¼ˆå‰è¿›ï¼‰ï¼Œè¿”å›0
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        asset_cfg: æœºå™¨äººèµ„äº§é…ç½®
        
    Returns:
        åé€€æƒ©ç½šï¼ˆæ­£å€¼è¡¨ç¤ºåé€€ç¨‹åº¦ï¼Œé…åˆè´Ÿweightä½¿ç”¨ï¼‰
    """
    asset: RigidObject = env.scene[asset_cfg.name]
    
    # è·å–æœºå™¨äººçš„çº¿é€Ÿåº¦ï¼ˆä¸–ç•Œåæ ‡ç³»ï¼‰
    velocity_w = asset.data.root_lin_vel_w  # (num_envs, 3)
    
    # è·å–æœºå™¨äººçš„æœå‘ï¼ˆä¸–ç•Œåæ ‡ç³»ï¼Œyawè§’å¯¹åº”çš„æ–¹å‘å‘é‡ï¼‰
    robot_heading = asset.data.heading_w  # (num_envs, 3)ï¼Œå·²å½’ä¸€åŒ–
    
    # è®¡ç®—é€Ÿåº¦åœ¨æœå‘ä¸Šçš„æŠ•å½±ï¼ˆæ­£å€¼=å‰è¿›ï¼Œè´Ÿå€¼=åé€€ï¼‰
    forward_speed = torch.sum(velocity_w * robot_heading, dim=1)  # (num_envs,)
    
    # å¦‚æœé€Ÿåº¦ä¸ºè´Ÿï¼ˆåé€€ï¼‰ï¼Œè¿”å›åé€€ç¨‹åº¦ï¼›å¦åˆ™è¿”å›0
    # ä½¿ç”¨ReLUç¡®ä¿åªæƒ©ç½šåé€€ï¼Œä¸å¥–åŠ±å‰è¿›
    backward_amount = torch.clamp(-forward_speed, min=0.0)  # åé€€é€Ÿåº¦çš„ç»å¯¹å€¼
    
    return backward_amount


def contact_force_penalty(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    threshold: float = 1.0,
) -> torch.Tensor:
    """è¿åŠ¨è¿‡ç¨‹ä¸­çš„ç¢°æ’æƒ©ç½šï¼ˆåŸºäºæ¥è§¦åŠ›ï¼‰ã€‚
    
    å®æ—¶ç›‘æµ‹æœºå™¨äººä¸ç¯å¢ƒçš„æ¥è§¦åŠ›ï¼Œå½“æ¥è§¦åŠ›è¶…è¿‡é˜ˆå€¼æ—¶ç»™äºˆæƒ©ç½šã€‚
    è¿™ä¸ç»ˆæ­¢æ¡ä»¶ä¸åŒï¼Œå¯ä»¥åœ¨è¿åŠ¨è¿‡ç¨‹ä¸­æŒç»­ç›‘æµ‹å¹¶æƒ©ç½šè½»å¾®ç¢°æ’ã€‚
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        sensor_cfg: æ¥è§¦åŠ›ä¼ æ„Ÿå™¨é…ç½®ï¼ˆå¦‚ "contact_forces"ï¼‰
        threshold: æ¥è§¦åŠ›é˜ˆå€¼ï¼ˆç‰›é¡¿ï¼‰ï¼Œè¶…è¿‡æ­¤å€¼åˆ™è®¤ä¸ºå‘ç”Ÿç¢°æ’
            é»˜è®¤ 1.0Nï¼Œä½äºç»ˆæ­¢æ¡ä»¶çš„é˜ˆå€¼ï¼ˆ5.0Nï¼‰
    
    Returns:
        ç¢°æ’æƒ©ç½šï¼ˆæ­£å€¼ï¼‰ï¼Œé…åˆè´Ÿweightä½¿ç”¨
    """
    # è·å–æ¥è§¦åŠ›ä¼ æ„Ÿå™¨
    contact_sensor = env.scene.sensors[sensor_cfg.name]
    
    # è·å–æŒ‡å®šéƒ¨ä½çš„æ¥è§¦åŠ› (num_envs, num_bodies, 3)
    contact_forces = contact_sensor.data.net_forces_w[:, sensor_cfg.body_ids]
    
    # è®¡ç®—æ¥è§¦åŠ›çš„æ¨¡ï¼ˆå¤§å°ï¼‰
    force_magnitudes = torch.norm(contact_forces, dim=-1)  # (num_envs, num_bodies)
    
    # å–æ‰€æœ‰ç›‘æµ‹éƒ¨ä½çš„æœ€å¤§æ¥è§¦åŠ›
    max_contact_force = torch.max(force_magnitudes, dim=-1)[0]  # (num_envs,)
    
    # è¶…è¿‡é˜ˆå€¼çš„éƒ¨åˆ†ä½œä¸ºæƒ©ç½š
    # penalty = max(0, force - threshold)
    penalty = torch.clamp(max_contact_force - threshold, min=0.0)
    
    # å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´ï¼Œé¿å…æƒ©ç½šè¿‡å¤§
    # å‡è®¾æœ€å¤§æ¥è§¦åŠ›ä¸è¶…è¿‡ 20N
    max_expected_force = 20.0
    penalty_normalized = torch.clamp(penalty / max_expected_force, max=1.0)
    
    return penalty_normalized


def obstacle_proximity_penalty(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    danger_distance: float = 0.3,
    warning_distance: float = 0.8,
) -> torch.Tensor:
    """åŸºäºLiDARçš„éšœç¢ç‰©æ¥è¿‘æƒ©ç½šã€‚
    
    å½“æœºå™¨äººè·ç¦»éšœç¢ç‰©è¿‡è¿‘æ—¶ç»™äºˆæƒ©ç½šï¼Œé¼“åŠ±ä¿æŒå®‰å…¨è·ç¦»ã€‚
    è¿™æ˜¯ä¸€ç§é¢„é˜²æ€§æƒ©ç½šï¼Œåœ¨å®é™…ç¢°æ’ä¹‹å‰å°±å¼€å§‹èµ·ä½œç”¨ã€‚
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        sensor_cfg: LiDARä¼ æ„Ÿå™¨é…ç½®ï¼ˆå¦‚ "obstacle_scanner"ï¼‰
        danger_distance: å±é™©è·ç¦»ï¼ˆç±³ï¼‰ï¼Œä½äºæ­¤è·ç¦»æƒ©ç½šæœ€å¤§
        warning_distance: è­¦å‘Šè·ç¦»ï¼ˆç±³ï¼‰ï¼Œé«˜äºæ­¤è·ç¦»æ— æƒ©ç½š
    
    Returns:
        æ¥è¿‘æƒ©ç½šï¼ˆæ­£å€¼ï¼‰ï¼Œé…åˆè´Ÿweightä½¿ç”¨
    """
    # è·å–LiDARä¼ æ„Ÿå™¨
    ray_caster: RayCaster = env.scene.sensors[sensor_cfg.name]
    
    # è·å–LiDARè·ç¦»æ•°æ® (num_envs, num_rays)
    distances = ray_caster.data.ray_hits_w[..., -1]
    
    # æ‰¾åˆ°æ¯ä¸ªç¯å¢ƒçš„æœ€å°è·ç¦»ï¼ˆæœ€è¿‘éšœç¢ç‰©ï¼‰
    min_distance = torch.min(distances, dim=-1)[0]  # (num_envs,)
    
    # è®¡ç®—æƒ©ç½š
    # - è·ç¦» >= warning_distance: penalty = 0
    # - danger_distance < è·ç¦» < warning_distance: çº¿æ€§æ’å€¼
    # - è·ç¦» <= danger_distance: penalty = 1.0
    penalty = torch.zeros_like(min_distance)
    
    # åœ¨è­¦å‘ŠåŒºé—´å†…ï¼Œçº¿æ€§æƒ©ç½š
    in_warning_zone = (min_distance >= danger_distance) & (min_distance < warning_distance)
    penalty[in_warning_zone] = (warning_distance - min_distance[in_warning_zone]) / (
        warning_distance - danger_distance
    )
    
    # åœ¨å±é™©åŒºé—´å†…ï¼Œæœ€å¤§æƒ©ç½š
    in_danger_zone = min_distance < danger_distance
    penalty[in_danger_zone] = 1.0
    
    return penalty
