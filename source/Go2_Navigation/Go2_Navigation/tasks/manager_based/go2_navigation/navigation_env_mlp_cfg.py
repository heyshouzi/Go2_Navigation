"""
Navigation environment configuration with MLP-based obstacle encoding.

This configuration uses a learnable MLP encoder to process raw lidar data,
instead of hand-crafted sector features.
"""

from isaaclab.envs import ManagerBasedRLEnvCfg
from isaaclab.managers import CurriculumTermCfg as CurrTerm
from isaaclab.managers import EventTermCfg as EventTerm
from isaaclab.managers import ObservationGroupCfg as ObsGroup
from isaaclab.managers import ObservationTermCfg as ObsTerm
from isaaclab.managers import RewardTermCfg as RewTerm
from isaaclab.managers import SceneEntityCfg
from isaaclab.managers import TerminationTermCfg as DoneTerm
from isaaclab.utils import configclass
from isaaclab.utils.assets import ISAACLAB_NUCLEUS_DIR
from isaaclab.terrains import (
    TerrainImporterCfg,
    TerrainGeneratorCfg,
    HfDiscreteObstaclesTerrainCfg,
)
from isaaclab.sensors import RayCasterCfg, patterns
from . import mdp
from isaaclab_tasks.manager_based.locomotion.velocity.config.go2.flat_env_cfg import (
    UnitreeGo2FlatEnvCfg,
)

LOW_LEVEL_ENV_CFG = UnitreeGo2FlatEnvCfg()
LOW_LEVEL_ENV_CFG.observations.policy.base_lin_vel = None


@configclass
class EventCfg:
    """Configuration for events."""

    reset_base = EventTerm(
        func=mdp.reset_root_state_uniform,
        mode="reset",
        params={
            "pose_range": {"x": (0, 0), "y": (0, 0), "yaw": (1.57, 1.57)},
            "velocity_range": {
                "x": (-0.0, 0.0),
                "y": (-0.0, 0.0),
                "z": (-0.0, 0.0),
                "roll": (-0.0, 0.0),
                "pitch": (-0.0, 0.0),
                "yaw": (-0.0, 0.0),
            },
        },
    )


@configclass
class ActionsCfg:
    """Action terms for the MDP."""

    pre_trained_policy_action: mdp.PreTrainedPolicyActionCfg = (
        mdp.PreTrainedPolicyActionCfg(
            asset_name="robot",
            policy_path=f"/home/wu/IsaacLab/logs/rsl_rl/unitree_go2_flat/2025-10-02_14-33-48/exported/policy.pt",
            # policy_path=f"/home/wu/Go2_Navigation/unitree_lab_policy.pt",git
            low_level_decimation=4,
            # low_level_actions=LOW_LEVEL_ENV_CFG.actions.JointPositionAction,  # ä½¿ç”¨æ–°é…ç½®ä¸­çš„ action åç§°
            low_level_actions=LOW_LEVEL_ENV_CFG.actions.joint_pos,
            low_level_observations=LOW_LEVEL_ENV_CFG.observations.policy,
        )
    )


@configclass
class ObservationsCfg:
    """Observation specifications for the MDP with MLP obstacle encoding."""

    @configclass
    class PolicyCfg(ObsGroup):
        """Observations for high-level navigation policy (Actor).

        ğŸ†• Uses 360-degree LiDAR matching real Unitree Go2 hardware!

        Observation structure:
        - pose_command (4): [x, y, z, heading] target pose in base frame
        - base_ang_vel (3): [wx, wy, wz] angular velocity
        - last_action (3): [vx, vy, vyaw] ä¸Šæ¬¡é‡‡å–çš„åŠ¨ä½œ
        - obstacle_features (359): ğŸ†• RAW 360Â° lidar ranges (NOT encoded here!)
          * 359 rays, one per degree (0Â° = forward, 90Â° = left, 180Â° = back, 270Â° = right)
          * Range: 0-30m (matching Unitree L1 LiDAR specs)
          * Encoded by ActorCriticWithLidarEncoder: 359 â†’ 36 dims (inside policy network)
          * Ensures proper gradient flow for end-to-end training

        âš ï¸ Total input to actor: 369 dims (4+3+3+359)
        âš ï¸ After policy's internal encoding: 4 + 3 + 3 + 36 = 46 dims

        âœ… Sim2Real: Identical to real Go2 LiDAR! No resampling needed in deployment.
        Note: base_lin_vel is removed from actor to improve robustness.
        """

        # 1. Target pose command
        pose_command = ObsTerm(
            func=mdp.generated_commands, params={"command_name": "pose_command"}
        )

        projected_gravity = ObsTerm(func=mdp.projected_gravity)

        # 4. ğŸ†• RAW 360Â° LiDAR data (will be encoded by policy network)
        obstacle_features = ObsTerm(
            func=mdp.obstacle_mlp_encoding,
            params={
                "sensor_cfg": SceneEntityCfg("obstacle_scanner"),
                "encoder_output_dim": 36,  # Target encoding dimension: 359 rays â†’ 36 features
            },
        )

    @configclass
    class CriticCfg(ObsGroup):
        """Observations for critic network.

        Critic has access to additional information (base_lin_vel) for better value estimation.

        Observation structure:
        - pose_command (4): [x, y, z, heading] target pose in base frame
        - base_lin_vel (3): [vx, vy, vz] linear velocity (only for critic)
        - projected_gravity (3): [gx, gy, gz] projected gravity
        - obstacle_features (359): RAW 360Â° lidar ranges
        âš ï¸ Total input to critic: 372 dims (4+3+3+3+359)
        âš ï¸ After policy's internal encoding: 4 + 3 + 3 + 3 + 36 = 49 dims
        """

        # 1. Target pose command
        pose_command = ObsTerm(
            func=mdp.generated_commands, params={"command_name": "pose_command"}
        )

        # 2. Current velocity state (including base_lin_vel for critic)
        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
        projected_gravity = ObsTerm(func=mdp.projected_gravity)

        # 4. ğŸ†• RAW 360Â° LiDAR data
        obstacle_features = ObsTerm(
            func=mdp.obstacle_mlp_encoding,
            params={
                "sensor_cfg": SceneEntityCfg("obstacle_scanner"),
                "encoder_output_dim": 36,
            },
        )

    # observation groups
    policy: PolicyCfg = PolicyCfg()
    critic: CriticCfg = CriticCfg()


@configclass
class RewardsCfg:
    """Reward terms for the MDP."""

    # Collision penalty (terminal - ä¸¥é‡ç¢°æ’å¯¼è‡´ç»ˆæ­¢)
    collision_penalty = RewTerm(
        func=mdp.is_terminated_term,
        weight=-2.0,
        params={"term_keys": ["base_contact"]},
    )

    # ğŸ†• è¿åŠ¨è¿‡ç¨‹ä¸­çš„ç¢°æ’æƒ©ç½šï¼ˆæŒç»­ç›‘æµ‹ï¼‰- è¯¾ç¨‹å­¦ä¹ ä»-1å¼€å§‹
    contact_force_penalty = RewTerm(
        func=mdp.contact_force_penalty,
        weight=-20.0,  # ğŸ“ è¯¾ç¨‹å­¦ä¹ ï¼šä»è½»æƒ©ç½šå¼€å§‹ï¼Œé€æ­¥å¢åŠ åˆ°-5.0
        params={
            "sensor_cfg": SceneEntityCfg(
                "contact_forces",
                body_names=[
                    "base",
                    "Head_upper",
                    "Head_lower",  # æœºèº«+å¤´éƒ¨
                    "FL_thigh",
                    "FR_thigh",
                    "RL_thigh",
                    "RR_thigh",  # å››æ¡å¤§è…¿
                ],
            ),
            "threshold": 1.0,  # 1N æ¥è§¦åŠ›é˜ˆå€¼ï¼ˆä½äºç»ˆæ­¢æ¡ä»¶çš„ 5Nï¼‰
        },
    )

    # ğŸ†• éšœç¢ç‰©æ¥è¿‘æƒ©ç½šï¼ˆé¢„é˜²æ€§ï¼‰
    obstacle_proximity_penalty = RewTerm(
        func=mdp.obstacle_proximity_penalty,
        weight=-1.0,
        params={
            "sensor_cfg": SceneEntityCfg("obstacle_scanner"),
            "danger_distance": 0.2,
            "warning_distance": 0.5,
        },
    )

    # Timeout penalty
    timeout_penalty = RewTerm(
        func=mdp.is_terminated_term,
        weight=-1.0,
        params={"term_keys": ["time_out"]},
    )

    # Position tracking (coarse-grained)
    position_tracking = RewTerm(
        func=mdp.position_command_error_tanh,
        weight=2.5,
        params={"std": 2.0, "command_name": "pose_command"},
    )

    # Position tracking (fine-grained)
    position_tracking_fine_grained = RewTerm(
        func=mdp.position_command_error_tanh,
        weight=1.5,
        params={"std": 0.5, "command_name": "pose_command"},
    )

    # Velocity smoothness
    velocity_smoothness = RewTerm(
        func=mdp.velocity_smoothness_penalty,
        weight=-0.5,
    )

    # Goal reached bonus
    goal_reached_bonus = RewTerm(
        func=mdp.goal_reached_bonus_time_aware,
        weight=10.0,
        params={
            "command_name": "pose_command",
            "distance_threshold": 0.3,
            "base_reward": 30.0,
            "time_bonus_weight": 20.0,
        },
    )


@configclass
class CommandsCfg:
    """Command terms for the MDP."""

    pose_command = mdp.UniformPose2dCommandCfg(
        asset_name="robot",
        simple_heading=False,
        resampling_time_range=(10.0, 10.0),
        debug_vis=True,
        ranges=mdp.UniformPose2dCommandCfg.Ranges(
            pos_x=(-7.0, 7.0),
            pos_y=(3.0, 4.0),  # ğŸ“ è¯¾ç¨‹å­¦ä¹ ï¼šåˆå§‹ä»ç¬¬ä¸€æ¡£å¼€å§‹ï¼ˆ3-4mï¼‰
            heading=(1.57, 1.57),
        ),
    )


@configclass
class TerminationsCfg:
    """Termination terms for the MDP."""

    time_out = DoneTerm(func=mdp.time_out, time_out=True)
    base_contact = DoneTerm(
        func=mdp.illegal_contact,
        params={
            "sensor_cfg": SceneEntityCfg("contact_forces", body_names="base"),
            "threshold": 5.0,
        },
    )

    goal_reached = DoneTerm(
        func=mdp.goal_reached,
        params={
            "command_name": "pose_command",
            "distance_threshold": 0.3,
        },
    )


@configclass
class CurriculumCfg:
    """Curriculum terms for the MDP."""

    # ğŸ“ æ¸è¿›å¼è·ç¦»è¯¾ç¨‹å­¦ä¹ ï¼šä»è¿‘åˆ°è¿œï¼ˆåŸºäºRLè¿­ä»£æ•°ï¼‰
    adaptive_distance = CurrTerm(
        func=mdp.modify_command_range_curriculum,
        params={
            "command_name": "pose_command",
            "attribute": "pos_y",
            "curriculum_levels": [
                (3.0, 4.0),  # ç¬¬ä¸€æ¡£ï¼šè¿‘è·ç¦»ï¼ˆ3-4mï¼‰- æœ€ç®€å•
                (5.0, 6.0),  # ç¬¬äºŒæ¡£ï¼šä¸­è·ç¦»ï¼ˆ5-6mï¼‰- ä¸­ç­‰éš¾åº¦
                (7.0, 8.0),  # ç¬¬ä¸‰æ¡£ï¼šè¿œè·ç¦»ï¼ˆ7-8mï¼‰- å›°éš¾
                (9.0, 10.0),  # ç¬¬å››æ¡£ï¼šè¶…è¿œè·ç¦»ï¼ˆ9-10mï¼‰- æœ€é«˜éš¾åº¦
            ],
            "success_thresholds": [
                0.60,
                0.70,
                0.80,
            ],  # å‡çº§é˜ˆå€¼ï¼š60%â†’æ¡£2ï¼Œ70%â†’æ¡£3ï¼Œ80%â†’æ¡£4
            "eval_interval": 10,  # æ¯10ä¸ªRL iterationè¯„ä¼°ä¸€æ¬¡
            "min_episodes_per_eval": 200,  # è‡³å°‘æ”¶é›†200ä¸ªepisodeå†è¯„ä¼°
            "warmup_iterations": 20,  # å‰20ä¸ªRL iterationä¸è¯„ä¼°
        },
    )

    adaptive_speed = CurrTerm(
        func=mdp.adaptive_speed_requirement,
        params={
            "success_threshold": 0.3,
            "low_speed_weight": 0.1,
            "high_speed_weight": 1.5,
            "eval_interval": 50,
        },
    )
    # # ğŸ“ ç¢°æ’æƒ©ç½šæƒé‡è¯¾ç¨‹å­¦ä¹ ï¼šå‰50ä¸ªRL iterationä¿æŒ-1ï¼Œç„¶åé€æ­¥å¢åŠ åˆ°-5
    # adaptive_collision_penalty = CurrTerm(
    #     func=mdp.adaptive_collision_penalty_curriculum,
    #     params={
    #         "term_name": "contact_force_penalty",
    #         "weight_levels": [-1.0, -3.0, -5.0],              # 3ä¸ªç­‰çº§ï¼šä»è½»åˆ°é‡
    #         "success_thresholds": [0.6, 0.8],                 # 2ä¸ªå‡çº§é˜ˆå€¼
    #         "eval_interval": 20,                               # æ¯20ä¸ªRL iterationè¯„ä¼°
    #         "min_episodes_per_eval": 200,                      # è‡³å°‘200ä¸ªepisode
    #         "warmup_iterations": 50,                           # å‰50ä¸ªRL iterationä¿æŒ-1æƒé‡
    #     },
    # )


@configclass
class NavigationEnvMLPCfg(ManagerBasedRLEnvCfg):
    """
    Navigation environment with MLP-based obstacle encoding.
    ğŸ†• New feature: Learnable obstacle representation
    - MLP encoder: 600 raw lidar points â†’ 17 learned features
    - Total observation: 4 + 3 + 3 + 17 = 27 dims
    - Encoder is trained end-to-end with the policy
    """

    # environment settings
    scene: SceneEntityCfg = LOW_LEVEL_ENV_CFG.scene
    actions: ActionsCfg = ActionsCfg()
    observations: ObservationsCfg = ObservationsCfg()
    events: EventCfg = EventCfg()
    # mdp settings
    commands: CommandsCfg = CommandsCfg()
    rewards: RewardsCfg = RewardsCfg()
    terminations: TerminationsCfg = TerminationsCfg()
    curriculum: CurriculumCfg = CurriculumCfg()

    def __post_init__(self):
        """Post initialization."""

        self.sim.dt = LOW_LEVEL_ENV_CFG.sim.dt
        self.sim.render_interval = LOW_LEVEL_ENV_CFG.decimation
        # decimationå†³å®šäº†ç¯å¢ƒä»¥å¤šå°‘ä»¿çœŸæ­¥ï¼ˆframeï¼‰ä¸ºå‘¨æœŸè¿›è¡Œä¸€æ¬¡å¤–éƒ¨RLCycleï¼ˆå³RLæ­¥é•¿ï¼‰ï¼Œ
        # æ¯”å¦‚decimationä¸º10æ—¶ï¼Œæ¯10ä¸ªç‰©ç†ä»¿çœŸæ­¥æ‰é‡‡é›†ä¸€æ¬¡RLè§‚æµ‹å¹¶åšä¸€æ¬¡åŠ¨ä½œå†³ç­–ã€å¥–åŠ±è®¡ç®—ç­‰ã€‚
        # è¿™æ ·å¯ä»¥å‡å°å¤–å±‚RLå†³ç­–çš„é¢‘ç‡ï¼Œæé«˜ä»¿çœŸæ•ˆç‡ï¼Œæ¨¡æ‹ŸçœŸå®æœºå™¨äººæ§åˆ¶å‘¨æœŸè¿œæ…¢äºä»¿çœŸæ­¥ï¼›
        # è¿™é‡Œä¹˜ä»¥10è¡¨ç¤ºæ¯10ä¸ªä½å±‚å†³ç­–å‘¨æœŸæ‰è¿›è¡Œä¸€æ¬¡é«˜å±‚RLç¯å¢ƒæ­¥ã€‚
        self.decimation = LOW_LEVEL_ENV_CFG.decimation * 2
        self.episode_length_s = self.commands.pose_command.resampling_time_range[1]

        if self.scene.height_scanner is not None:
            self.scene.height_scanner.update_period = (
                self.actions.pre_trained_policy_action.low_level_decimation
                * self.sim.dt
            )
        if self.scene.contact_forces is not None:
            self.scene.contact_forces.update_period = self.sim.dt

        # RayCaster sensor for obstacle detection
        self.scene.obstacle_scanner = RayCasterCfg(
            prim_path="{ENV_REGEX_NS}/Robot/base",
            offset=RayCasterCfg.OffsetCfg(
                pos=(0.0, 0.0, 0.1)
            ),  # LiDARå®‰è£…ä½ç½®ï¼ˆæœºèº«é¡¶éƒ¨ï¼‰
            ray_alignment="yaw",
            pattern_cfg=patterns.LidarPatternCfg(
                channels=1,  # å•çº¿LiDAR
                vertical_fov_range=(0.0, 0.0),  # å•çº¿ï¼šå‚ç›´FOVä¸º0
                horizontal_fov_range=(0.0, 360.0),  # 360åº¦æ°´å¹³æ‰«æ
                horizontal_res=1.0,  # æ¯åº¦1æ¡å°„çº¿
                # æ³¨æ„ï¼š360åº¦æ‰«æä¼šæ’é™¤æœ€åä¸€ä¸ªç‚¹ï¼ˆ360Â°ä¸0Â°é‡å¤ï¼‰
                # å®é™…ç”Ÿæˆ 359 æ¡å°„çº¿ï¼š[0Â°, 1Â°, 2Â°, ..., 358Â°]
            ),
            max_distance=8.0,
            drift_range=(-0.0, 0.0),
            debug_vis=False,  # è®­ç»ƒæ—¶å…³é—­å¯è§†åŒ–ä»¥æé«˜é€Ÿåº¦
            mesh_prim_paths=["/World/ground"],
        )

        self.scene.num_envs = 1024
        self.scene.sky_light = AssetBaseCfg(
            prim_path="/World/skyLight",
            spawn=sim_utils.DomeLightCfg(
                intensity=750.0,
                texture_file=f"{ISAACLAB_NUCLEUS_DIR}/Materials/Textures/Skies/PolyHaven/kloofendal_43d_clear_puresky_4k.hdr",
            ),
        )
        self.scene.terrain = TerrainImporterCfg(
            num_envs=self.scene.num_envs,
            env_spacing=0.0,
            prim_path="/World/ground",
            terrain_type="generator",
            terrain_generator=TerrainGeneratorCfg(
                seed=42,
                size=(20.0, 20.0),
                border_width=20.0,
                num_rows=1,
                num_cols=1,
                horizontal_scale=0.1,
                vertical_scale=0.1,
                slope_threshold=0.75,
                use_cache=False,
                color_scheme="height",
                sub_terrains={
                    "obstacles": HfDiscreteObstaclesTerrainCfg(
                        size=(20.0, 20.0),
                        horizontal_scale=0.1,
                        vertical_scale=0.1,
                        border_width=0.0,
                        num_obstacles=70,
                        obstacle_height_mode="fixed",
                        obstacle_width_range=(0.4, 0.8),
                        obstacle_height_range=(0.3, 1.0),
                        platform_width=2.0,
                    ),
                },
            ),
        )


from isaaclab.assets import AssetBaseCfg
import isaaclab.sim as sim_utils


class NavigationEnvMLPCfg_PLAY(NavigationEnvMLPCfg):
    def __post_init__(self) -> None:
        # post init of parent
        super().__post_init__()

        # make a smaller scene for play
        self.scene.num_envs = 1
        self.scene.env_spacing = 2.5
        # disable randomization for play
        self.observations.policy.enable_corruption = False
        self.scene.obstacle_scanner.debug_vis = True
        self.episode_length_s = 15.0
        # ğŸ® Playæ¨¡å¼ï¼šæµ‹è¯•å…¨è·ç¦»èŒƒå›´ï¼ˆ3-10mï¼‰
        self.commands.pose_command.resampling_time_range = (15.0, 15.0)
        self.commands.pose_command.ranges.pos_x = (-2.0, -2.0)
        self.commands.pose_command.ranges.pos_y = (7.0, 7.0)
        # self.events.reset_base.params["pose_range"]["y"] = (-6.0, -6.0)
        # self.events.reset_base.params["pose_range"]["x"] = (-2.0, 2.0)

        # ------------------------------------------------------
        # ğŸ’¡ æ·»åŠ å…‰æº (IsaacLab 2.3)
        # ------------------------------------------------------
        self.scene.light = AssetBaseCfg(
            prim_path="/World/light",
            spawn=sim_utils.DistantLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75)),
            init_state=AssetBaseCfg.InitialStateCfg(pos=(0.0, 0.0, 500.0)),
        )
