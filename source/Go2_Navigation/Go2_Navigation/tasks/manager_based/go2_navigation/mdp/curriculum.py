# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""Custom curriculum functions for navigation tasks."""

from __future__ import annotations

import torch
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


def adaptive_speed_requirement(
    env: ManagerBasedRLEnv,
    env_ids: torch.Tensor,
    success_threshold: float = 0.4,
    low_speed_weight: float = 0.3,
    high_speed_weight: float = 2.0,
    eval_interval: int = 50,  # æ¯50ä¸ªiterationè¯„ä¼°ä¸€æ¬¡
) -> dict[str, float]:
    """æ ¹æ®æˆåŠŸç‡è‡ªé€‚åº”è°ƒæ•´é€Ÿåº¦è¦æ±‚ã€‚
    
    ç›‘æ§æˆåŠŸç‡ï¼ŒåŠ¨æ€è°ƒæ•´time_efficiencyçš„æƒé‡ï¼š
    - æˆåŠŸç‡ < 40%ï¼šé™ä½é€Ÿåº¦è¦æ±‚ï¼ˆweight = 0.3ï¼‰
    - æˆåŠŸç‡ â‰¥ 40%ï¼šæé«˜é€Ÿåº¦è¦æ±‚ï¼ˆweight = 2.0ï¼‰
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        env_ids: ç¯å¢ƒIDï¼ˆcurriculumå‡½æ•°å¿…é¡»æ¥å—ä½†å¯èƒ½ä¸ç”¨ï¼‰
        success_threshold: æˆåŠŸç‡é˜ˆå€¼ï¼ˆé»˜è®¤0.4 = 40%ï¼‰
        low_speed_weight: ä½é€Ÿåº¦è¦æ±‚çš„æƒé‡
        high_speed_weight: é«˜é€Ÿåº¦è¦æ±‚çš„æƒé‡
        eval_interval: è¯„ä¼°é—´éš”ï¼ˆæ¯Nä¸ªiterationè¯„ä¼°ä¸€æ¬¡ï¼‰
        
    Returns:
        åŒ…å«å½“å‰çŠ¶æ€çš„å­—å…¸ï¼Œç”¨äºæ—¥å¿—è®°å½•
    """
    # åˆå§‹åŒ–çŠ¶æ€å˜é‡
    if not hasattr(env, '_speed_curriculum_state'):
        env._speed_curriculum_state = {
            'current_weight': low_speed_weight,
            'iteration_count': 0,
            'success_count': 0,
            'episode_count': 0,
        }
    
    state = env._speed_curriculum_state
    state['iteration_count'] += 1
    
    # ç»Ÿè®¡æœ¬iterationçš„æˆåŠŸå’Œæ€»episodeæ•°
    # é€šè¿‡æ£€æŸ¥å“ªäº›ç¯å¢ƒåˆšresetæ¥ç»Ÿè®¡
    if hasattr(env, 'reset_buf') and env.reset_buf is not None:
        # reset_bufä¸ºTrueè¡¨ç¤ºè¿™ä¸ªç¯å¢ƒåˆšç»ˆæ­¢
        terminated_envs = env.reset_buf.sum().item()
        state['episode_count'] += terminated_envs
        
        # ç»Ÿè®¡å…¶ä¸­æœ‰å¤šå°‘æ˜¯æˆåŠŸçš„
        if terminated_envs > 0 and hasattr(env.termination_manager, 'get_term'):
            try:
                goal_reached = env.termination_manager.get_term('goal_reached')
                success_envs = (goal_reached & env.reset_buf).sum().item()
                state['success_count'] += success_envs
            except:
                pass  # å¦‚æœè·å–å¤±è´¥ï¼Œè·³è¿‡
    
    # æ¯Nä¸ªiterationsè¯„ä¼°ä¸€æ¬¡
    if state['iteration_count'] % eval_interval == 0:
        # è®¡ç®—æˆåŠŸç‡
        if state['episode_count'] > 0:
            success_rate = state['success_count'] / state['episode_count']
        else:
            success_rate = 0.0
        
        # æ ¹æ®æˆåŠŸç‡å†³å®šç›®æ ‡æƒé‡
        if success_rate >= success_threshold:
            target_weight = high_speed_weight
            status = "high_speed"
        else:
            target_weight = low_speed_weight
            status = "low_speed"
        
        # å¹³æ»‘è¿‡æ¸¡ï¼ˆé¿å…çªç„¶è·³å˜ï¼‰
        current = state['current_weight']
        if current != target_weight:
            # æ¯æ¬¡è°ƒæ•´20%çš„å·®è·
            state['current_weight'] = current + (target_weight - current) * 0.2
        
        # é‡ç½®è®¡æ•°å™¨ï¼ˆä¸ºä¸‹ä¸€ä¸ªå‘¨æœŸå‡†å¤‡ï¼‰
        state['success_count'] = 0
        state['episode_count'] = 0
        
        # æ›´æ–°reward managerä¸­çš„æƒé‡
        if hasattr(env, 'reward_manager'):
            for i, term_name in enumerate(env.reward_manager._term_names):
                if 'time_efficiency' in term_name:
                    env.reward_manager._term_cfgs[i].weight = state['current_weight']
                    break
        
        # è¿”å›çŠ¶æ€ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        return {
            "success_rate": success_rate,
            "speed_weight": state['current_weight'],
            "status": 1.0 if status == "high_speed" else 0.0,
        }
    
    # éè¯„ä¼°iterationï¼Œè¿”å›å½“å‰çŠ¶æ€
    return {
        "speed_weight": state['current_weight'],
    }


def adaptive_terrain_difficulty(
    env: ManagerBasedRLEnv,
    env_ids: torch.Tensor,
    easy_threshold: float = 0.3,
    hard_threshold: float = 0.6,
    obstacle_range: tuple[int, int, int] = (20, 50, 80),  # ç®€å•ã€ä¸­ç­‰ã€å›°éš¾
    height_range: tuple[tuple[float, float], tuple[float, float], tuple[float, float]] = (
        (0.5, 1.5),  # ç®€å•ï¼šä½éšœç¢ç‰©
        (1.0, 3.0),  # ä¸­ç­‰ï¼šä¸­ç­‰éšœç¢ç‰©
        (1.5, 4.5),  # å›°éš¾ï¼šé«˜éšœç¢ç‰©
    ),
    eval_interval: int = 50,
) -> dict[str, float]:
    """æ ¹æ®æˆåŠŸç‡è‡ªé€‚åº”è°ƒæ•´åœ°å½¢éš¾åº¦ã€‚
    
    åŠ¨æ€è°ƒæ•´éšœç¢ç‰©æ•°é‡å’Œé«˜åº¦ï¼š
    - æˆåŠŸç‡ < 30%ï¼šç®€å•åœ°å½¢ï¼ˆ20ä¸ªéšœç¢ç‰©ï¼Œ0.5-1.5mé«˜ï¼‰
    - æˆåŠŸç‡ 30-60%ï¼šä¸­ç­‰åœ°å½¢ï¼ˆ50ä¸ªéšœç¢ç‰©ï¼Œ1.0-3.0mé«˜ï¼‰
    - æˆåŠŸç‡ > 60%ï¼šå›°éš¾åœ°å½¢ï¼ˆ80ä¸ªéšœç¢ç‰©ï¼Œ1.5-4.5mé«˜ï¼‰
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        env_ids: ç¯å¢ƒID
        easy_threshold: ç®€å•/ä¸­ç­‰çš„åˆ†ç•Œçº¿ï¼ˆé»˜è®¤0.3 = 30%ï¼‰
        hard_threshold: ä¸­ç­‰/å›°éš¾çš„åˆ†ç•Œçº¿ï¼ˆé»˜è®¤0.6 = 60%ï¼‰
        obstacle_range: (ç®€å•æ•°é‡, ä¸­ç­‰æ•°é‡, å›°éš¾æ•°é‡)
        height_range: ((ç®€å•min, ç®€å•max), (ä¸­ç­‰min, ä¸­ç­‰max), (å›°éš¾min, å›°éš¾max))
        eval_interval: è¯„ä¼°é—´éš”
        
    Returns:
        çŠ¶æ€å­—å…¸
    """
    # åˆå§‹åŒ–çŠ¶æ€
    if not hasattr(env, '_terrain_curriculum_state'):
        env._terrain_curriculum_state = {
            'iteration_count': 0,
            'success_count': 0,
            'episode_count': 0,
            'current_difficulty': 0,  # 0=ç®€å•, 1=ä¸­ç­‰, 2=å›°éš¾
            'current_obstacle_count': obstacle_range[0],
            'current_height_min': height_range[0][0],
            'current_height_max': height_range[0][1],
        }
    
    state = env._terrain_curriculum_state
    state['iteration_count'] += 1
    
    # ç»Ÿè®¡æˆåŠŸç‡ï¼ˆä¸é€Ÿåº¦è¯¾ç¨‹å…±äº«é€»è¾‘ï¼‰
    if hasattr(env, 'reset_buf') and env.reset_buf is not None:
        terminated_envs = env.reset_buf.sum().item()
        state['episode_count'] += terminated_envs
        
        if terminated_envs > 0 and hasattr(env.termination_manager, 'get_term'):
            try:
                goal_reached = env.termination_manager.get_term('goal_reached')
                success_envs = (goal_reached & env.reset_buf).sum().item()
                state['success_count'] += success_envs
            except:
                pass
    
    # æ¯Nä¸ªiterationsè¯„ä¼°ä¸€æ¬¡
    if state['iteration_count'] % eval_interval == 0:
        # è®¡ç®—æˆåŠŸç‡
        if state['episode_count'] > 0:
            success_rate = state['success_count'] / state['episode_count']
        else:
            success_rate = 0.0
        
        # æ ¹æ®æˆåŠŸç‡å†³å®šç›®æ ‡éš¾åº¦
        if success_rate < easy_threshold:
            target_difficulty = 0  # ç®€å•
            target_obstacles = obstacle_range[0]
            target_height = height_range[0]
            status = "easy"
        elif success_rate < hard_threshold:
            target_difficulty = 1  # ä¸­ç­‰
            target_obstacles = obstacle_range[1]
            target_height = height_range[1]
            status = "medium"
        else:
            target_difficulty = 2  # å›°éš¾
            target_obstacles = obstacle_range[2]
            target_height = height_range[2]
            status = "hard"
        
        # åªåœ¨éš¾åº¦å˜åŒ–æ—¶æ›´æ–°
        if target_difficulty != state['current_difficulty']:
            # å¹³æ»‘è¿‡æ¸¡éšœç¢ç‰©æ•°é‡
            current_obs = state['current_obstacle_count']
            diff_obs = target_obstacles - current_obs
            state['current_obstacle_count'] = int(current_obs + diff_obs * 0.3)
            
            # å¹³æ»‘è¿‡æ¸¡é«˜åº¦èŒƒå›´
            current_h_min = state['current_height_min']
            current_h_max = state['current_height_max']
            diff_h_min = target_height[0] - current_h_min
            diff_h_max = target_height[1] - current_h_max
            state['current_height_min'] = current_h_min + diff_h_min * 0.3
            state['current_height_max'] = current_h_max + diff_h_max * 0.3
            
            state['current_difficulty'] = target_difficulty
        
        # é‡ç½®è®¡æ•°å™¨
        state['success_count'] = 0
        state['episode_count'] = 0
        
        # è¿”å›çŠ¶æ€
        return {
            "success_rate": success_rate,
            "difficulty": float(state['current_difficulty']),
            "obstacle_count": float(state['current_obstacle_count']),
            "height_min": state['current_height_min'],
            "height_max": state['current_height_max'],
            "status": float(target_difficulty),
        }
    
    # éè¯„ä¼°iteration
    return {
        "difficulty": float(state['current_difficulty']),
        "obstacle_count": float(state['current_obstacle_count']),
    }


def adaptive_collision_penalty_curriculum(
    env: ManagerBasedRLEnv,
    env_ids: torch.Tensor,
    term_name: str = "contact_force_penalty",
    weight_levels: list[float] = [-1.0,  -3.0,  -5.0],  # ä»è½»åˆ°é‡çš„æƒ©ç½šæƒé‡
    success_thresholds: list[float] = [0.6, 0.8, 0.9],  # å‡çº§é˜ˆå€¼
    eval_interval: int = 100,
    min_episodes_per_eval: int = 200,
    warmup_iterations: int = 400,  # å‰400ä¸ªå›åˆä¿æŒ-1æƒé‡
) -> dict[str, float]:
    """æ ¹æ®æˆåŠŸç‡è‡ªé€‚åº”è°ƒæ•´ç¢°æ’æƒ©ç½šæƒé‡ã€‚
    
    å®ç°ç¢°æ’æƒ©ç½šæƒé‡çš„è¯¾ç¨‹å­¦ä¹ ï¼š
    - Level 0: è½»æƒ©ç½š (-1.0) - è®©æœºå™¨äººå…ˆå­¦ä¼šåŸºæœ¬å¯¼èˆª
    - Level 1: è¾ƒé‡æƒ©ç½š (-3.0) - å¼€å§‹å…³æ³¨ç¢°æ’
    - Level 2: é‡æƒ©ç½š (-5.0) - è¿›ä¸€æ­¥å‡å°‘ç¢°æ’
    
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        env_ids: ç¯å¢ƒID
        term_name: å¥–åŠ±é¡¹åç§°ï¼ˆé»˜è®¤ "contact_force_penalty"ï¼‰
        weight_levels: æƒé‡ç­‰çº§åˆ—è¡¨ï¼Œä»è½»åˆ°é‡
        success_thresholds: å‡çº§é˜ˆå€¼åˆ—è¡¨ï¼Œé•¿åº¦ä¸º len(weight_levels) - 1
        eval_interval: è¯„ä¼°é—´éš”ï¼ˆæ¯Nä¸ªiterationè¯„ä¼°ä¸€æ¬¡ï¼‰
        min_episodes_per_eval: æœ€å°‘æ ·æœ¬é‡
        warmup_iterations: é¢„çƒ­æœŸ
        
    Returns:
        çŠ¶æ€å­—å…¸ï¼Œç”¨äºæ—¥å¿—è®°å½•
    """
    # åˆå§‹åŒ–çŠ¶æ€
    if not hasattr(env, '_collision_curriculum_state'):
        env._collision_curriculum_state = {
            'last_rl_iteration': 0,
            'success_count': 0,
            'episode_count': 0,
            'current_level': 0,  # ä»æœ€è½»æƒ©ç½šå¼€å§‹
            'current_weight': weight_levels[0],
            'initialized_at_iteration': 0,
        }
        print(f"ğŸ“ [Collision Curriculum] åˆå§‹åŒ–å®Œæˆï¼Œåˆå§‹æƒé‡: {weight_levels[0]}")
    
    state = env._collision_curriculum_state
    
    # ç»Ÿè®¡æˆåŠŸç‡ï¼ˆæ¯æ¬¡ç¯å¢ƒé‡ç½®æ—¶ç´¯ç§¯ï¼‰
    if hasattr(env, 'reset_buf') and env.reset_buf is not None:
        terminated_envs = env.reset_buf.sum().item()
        state['episode_count'] += terminated_envs
        
        if terminated_envs > 0 and hasattr(env.termination_manager, 'get_term'):
            try:
                goal_reached = env.termination_manager.get_term('goal_reached')
                success_envs = (goal_reached & env.reset_buf).sum().item()
                state['success_count'] += success_envs
            except:
                pass
    
    # ğŸ”§ ä½¿ç”¨RLè®­ç»ƒè¿­ä»£æ•°
    current_rl_iteration = 0
    if hasattr(env, 'rl_iteration'):
        current_rl_iteration = env.rl_iteration
    elif hasattr(env, 'learning_iteration'):
        current_rl_iteration = env.learning_iteration
    elif hasattr(env, 'train_iteration'):
        current_rl_iteration = env.train_iteration
    else:
        decimation = getattr(env, 'decimation', 16)
        current_rl_iteration = env.common_step_counter // decimation
    
    iterations_since_last_eval = current_rl_iteration - state['last_rl_iteration']
    
    # é¢„çƒ­é˜¶æ®µï¼šè·³è¿‡è¯„ä¼°ï¼Œå…ˆç´¯ç§¯æ•°æ®ï¼Œç¡®ä¿æƒé‡ä¿æŒä¸º-1
    if current_rl_iteration - state['initialized_at_iteration'] < warmup_iterations:
        # åœ¨é¢„çƒ­æœŸå†…ï¼Œç¡®ä¿æƒé‡ä¿æŒä¸ºåˆå§‹å€¼ï¼ˆ-1.0ï¼‰
        if state['current_weight'] != weight_levels[0]:
            state['current_weight'] = weight_levels[0]
            # æ›´æ–°reward managerä¸­çš„æƒé‡
            if hasattr(env, 'reward_manager'):
                try:
                    term_cfg = env.reward_manager.get_term_cfg(term_name)
                    term_cfg.weight = state['current_weight']
                    env.reward_manager.set_term_cfg(term_name, term_cfg)
                    print(f"ğŸ”§ [Collision Curriculum] é¢„çƒ­æœŸå¼ºåˆ¶ä¿æŒæƒé‡: {state['current_weight']}")
                except Exception as e:
                    print(f"âš ï¸ é¢„çƒ­æœŸæ›´æ–°æƒé‡å¤±è´¥: {e}")
        
        return {
            "current_level": float(state['current_level']),
            "current_weight": state['current_weight'],
            "warmup": float(warmup_iterations - (current_rl_iteration - state['initialized_at_iteration'])),
        }

    # æ¯Nä¸ªRL iterationsè¯„ä¼°ä¸€æ¬¡ï¼Œä¸”ä¿è¯æ ·æœ¬é‡è¶³å¤Ÿ
    if iterations_since_last_eval >= eval_interval and state['episode_count'] >= min_episodes_per_eval:
        # è®¡ç®—æˆåŠŸç‡
        if state['episode_count'] > 0:
            success_rate = state['success_count'] / state['episode_count']
        else:
            success_rate = 0.0
        
        # åˆ¤æ–­æ˜¯å¦å‡çº§
        current_level = state['current_level']
        max_level = len(weight_levels) - 1
        
        # å¦‚æœå½“å‰ä¸æ˜¯æœ€é«˜éš¾åº¦ï¼Œä¸”è¾¾åˆ°äº†å‡çº§é˜ˆå€¼
        if current_level < max_level and success_rate >= success_thresholds[current_level]:
            # å‡çº§åˆ°ä¸‹ä¸€æ¡£
            new_level = current_level + 1
            state['current_level'] = new_level
            state['current_weight'] = weight_levels[new_level]
            
            # æ›´æ–°reward managerä¸­çš„æƒé‡
            if hasattr(env, 'reward_manager'):
                try:
                    term_cfg = env.reward_manager.get_term_cfg(term_name)
                    old_weight = term_cfg.weight
                    term_cfg.weight = state['current_weight']
                    env.reward_manager.set_term_cfg(term_name, term_cfg)
                    
                    print(f"\nğŸ“ [Collision Curriculum] æƒ©ç½šæƒé‡å‡çº§ï¼")
                    print(f"   å½“å‰RL iteration: {current_rl_iteration}")
                    print(f"   Level {current_level} â†’ Level {new_level}")
                    print(f"   æˆåŠŸç‡: {success_rate:.2%} (é˜ˆå€¼: {success_thresholds[current_level]:.2%})")
                    print(f"   æƒé‡å˜åŒ–: {old_weight} â†’ {state['current_weight']}")
                except Exception as e:
                    print(f"âš ï¸ æ›´æ–°ç¢°æ’æƒ©ç½šæƒé‡å¤±è´¥: {e}")
        
        # é‡ç½®è®¡æ•°å™¨
        state['success_count'] = 0
        state['episode_count'] = 0
        state['last_rl_iteration'] = current_rl_iteration
        
        # è¿”å›çŠ¶æ€
        return {
            "success_rate": success_rate,
            "current_level": float(state['current_level']),
            "current_weight": state['current_weight'],
            "iterations_since_eval": iterations_since_last_eval,
        }
    
    # éè¯„ä¼°iteration æˆ– æ ·æœ¬ä¸è¶³
    return {
        "current_level": float(state['current_level']),
        "current_weight": state['current_weight'],
        "episodes_collected": float(state['episode_count']),
    }


def modify_command_range_curriculum(
    env: ManagerBasedRLEnv,
    env_ids: torch.Tensor,
    command_name: str,
    attribute: str,
    curriculum_levels: list[tuple[float, float]],
    success_thresholds: list[float],
    eval_interval: int = 100,
    min_episodes_per_eval: int = 200,  # æœ€å°‘æ ·æœ¬é‡ï¼šé¿å…è¿‡æ—©å‡çº§
    warmup_iterations: int = 10,       # é¢„çƒ­ï¼šå‰è‹¥å¹²iterationä¸è¯„ä¼°
) -> dict[str, float]:
    """æ ¹æ®æˆåŠŸç‡æ¸è¿›å¼è°ƒæ•´å‘½ä»¤é‡‡æ ·èŒƒå›´ã€‚
    
    å®ç°ä¸‰æ¡£è·ç¦»è¯¾ç¨‹å­¦ä¹ ï¼š
    - Level 0 (æœ€ç®€å•): è¾¾åˆ° threshold[0] çš„æˆåŠŸç‡åè¿›å…¥ Level 1
    - Level 1 (ä¸­ç­‰): è¾¾åˆ° threshold[1] çš„æˆåŠŸç‡åè¿›å…¥ Level 2
    - Level 2 (å›°éš¾): æœ€ç»ˆç›®æ ‡
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        env_ids: ç¯å¢ƒID
        command_name: å‘½ä»¤åç§°ï¼ˆå¦‚ "pose_command"ï¼‰
        attribute: è¦ä¿®æ”¹çš„å±æ€§åï¼ˆå¦‚ "pos_y"ï¼‰
        curriculum_levels: è¯¾ç¨‹ç­‰çº§åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (min, max) å…ƒç»„
            ä¾‹å¦‚: [(3.0, 4.0), (5.0, 6.0), (7.0, 8.0)]
        success_thresholds: æˆåŠŸç‡é˜ˆå€¼åˆ—è¡¨ï¼Œé•¿åº¦ä¸º len(curriculum_levels) - 1
            ä¾‹å¦‚: [0.70, 0.75] è¡¨ç¤º70%è¿›å…¥Level1ï¼Œ75%è¿›å…¥Level2
        eval_interval: è¯„ä¼°é—´éš”ï¼ˆæ¯Nä¸ªiterationè¯„ä¼°ä¸€æ¬¡ï¼‰
        
    Returns:
        çŠ¶æ€å­—å…¸ï¼Œç”¨äºæ—¥å¿—è®°å½•
    """
    # åˆå§‹åŒ–çŠ¶æ€
    if not hasattr(env, '_distance_curriculum_state'):
        env._distance_curriculum_state = {
            'last_rl_iteration': 0,  # ä¸Šæ¬¡è¯„ä¼°æ—¶çš„RLè¿­ä»£æ•°
            'success_count': 0,
            'episode_count': 0,
            'current_level': 0,  # ä»ç¬¬ä¸€æ¡£å¼€å§‹
            'current_range': curriculum_levels[0],
            'initialized_at_iteration': 0,  # åˆå§‹åŒ–çš„RLè¿­ä»£æ•°
        }
        print(f"ğŸ“ [Distance Curriculum] åˆå§‹åŒ–å®Œæˆï¼Œåˆå§‹èŒƒå›´: {curriculum_levels[0]}")
    
    state = env._distance_curriculum_state
    
    # ç»Ÿè®¡æˆåŠŸç‡ï¼ˆæ¯æ¬¡ç¯å¢ƒé‡ç½®æ—¶ç´¯ç§¯ï¼‰
    if hasattr(env, 'reset_buf') and env.reset_buf is not None:
        terminated_envs = env.reset_buf.sum().item()
        state['episode_count'] += terminated_envs
        
        if terminated_envs > 0 and hasattr(env.termination_manager, 'get_term'):
            try:
                goal_reached = env.termination_manager.get_term('goal_reached')
                success_envs = (goal_reached & env.reset_buf).sum().item()
                state['success_count'] += success_envs
            except:
                pass
    
    # ğŸ”§ ä½¿ç”¨RLè®­ç»ƒè¿­ä»£æ•°ï¼ˆä»ç¯å¢ƒå±æ€§è·å–ï¼‰
    # å°è¯•ä»ä¸åŒå¯èƒ½çš„å±æ€§è·å–RLè¿­ä»£æ•°
    current_rl_iteration = 0
    if hasattr(env, 'rl_iteration'):
        current_rl_iteration = env.rl_iteration
    elif hasattr(env, 'learning_iteration'):
        current_rl_iteration = env.learning_iteration
    elif hasattr(env, 'train_iteration'):
        current_rl_iteration = env.train_iteration
    else:
        # å¦‚æœæ‰¾ä¸åˆ°RLè¿­ä»£æ•°ï¼Œå›é€€åˆ°ç¯å¢ƒæ­¥æ•°ï¼ˆé™¤ä»¥decimationä¼°ç®—ï¼‰
        decimation = getattr(env, 'decimation', 16)
        current_rl_iteration = env.common_step_counter // decimation
    
    iterations_since_last_eval = current_rl_iteration - state['last_rl_iteration']
    
    # é¢„çƒ­é˜¶æ®µï¼šè·³è¿‡è¯„ä¼°ï¼Œå…ˆç´¯ç§¯æ•°æ®
    if current_rl_iteration - state['initialized_at_iteration'] < warmup_iterations:
        return {
            "current_level": float(state['current_level']),
            "range_min": state['current_range'][0],
            "range_max": state['current_range'][1],
            "warmup": float(warmup_iterations - (current_rl_iteration - state['initialized_at_iteration'])),
        }

    # æ¯Nä¸ªRL iterationsè¯„ä¼°ä¸€æ¬¡ï¼Œä¸”ä¿è¯æ ·æœ¬é‡è¶³å¤Ÿ
    if iterations_since_last_eval >= eval_interval and state['episode_count'] >= min_episodes_per_eval:
        # è®¡ç®—æˆåŠŸç‡
        if state['episode_count'] > 0:
            success_rate = state['success_count'] / state['episode_count']
        else:
            success_rate = 0.0
        
        # åˆ¤æ–­æ˜¯å¦å‡çº§
        current_level = state['current_level']
        max_level = len(curriculum_levels) - 1
        
        # å¦‚æœå½“å‰ä¸æ˜¯æœ€é«˜éš¾åº¦ï¼Œä¸”è¾¾åˆ°äº†å‡çº§é˜ˆå€¼
        if current_level < max_level and success_rate >= success_thresholds[current_level]:
            # å‡çº§åˆ°ä¸‹ä¸€æ¡£
            new_level = current_level + 1
            state['current_level'] = new_level
            state['current_range'] = curriculum_levels[new_level]
            
            # æ›´æ–°å‘½ä»¤ç®¡ç†å™¨ä¸­çš„èŒƒå›´
            if hasattr(env, 'command_manager'):
                cmd_term = env.command_manager.get_term(command_name)
                if cmd_term is not None and hasattr(cmd_term, 'cfg'):
                    # åŠ¨æ€ä¿®æ”¹é…ç½®ä¸­çš„èŒƒå›´
                    if hasattr(cmd_term.cfg.ranges, attribute):
                        setattr(cmd_term.cfg.ranges, attribute, state['current_range'])
                        print(f"\nğŸ“ [Distance Curriculum] è·ç¦»å‡çº§ï¼")
                        print(f"   å½“å‰RL iteration: {current_rl_iteration}")
                        print(f"   Level {current_level} â†’ Level {new_level}")
                        print(f"   æˆåŠŸç‡: {success_rate:.2%} (é˜ˆå€¼: {success_thresholds[current_level]:.2%})")
                        print(f"   æ”¶é›†çš„episodeæ•°: {state['episode_count']}")
                        print(f"   æ–°çš„ {attribute} èŒƒå›´: {state['current_range']}")
        
        # é‡ç½®è®¡æ•°å™¨
        state['success_count'] = 0
        state['episode_count'] = 0
        state['last_rl_iteration'] = current_rl_iteration  # ğŸ”§ æ›´æ–°ä¸Šæ¬¡è¯„ä¼°çš„RLè¿­ä»£æ•°
        
        # è¿”å›çŠ¶æ€
        return {
            "success_rate": success_rate,
            "current_level": float(state['current_level']),
            "range_min": state['current_range'][0],
            "range_max": state['current_range'][1],
            "iterations_since_eval": iterations_since_last_eval,  # æ·»åŠ åˆ°æ—¥å¿—
        }
    
    # éè¯„ä¼°iteration æˆ– æ ·æœ¬ä¸è¶³
    return {
        "current_level": float(state['current_level']),
        "range_min": state['current_range'][0],
        "range_max": state['current_range'][1],
        "episodes_collected": float(state['episode_count']),
    }

